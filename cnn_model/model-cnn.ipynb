{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTvP0IOKmJPN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVgGWWvrmJCe"
      },
      "source": [
        "### Loading the Augmented Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r9Lb90YGl9nN"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rciyiaYQl-0C"
      },
      "outputs": [],
      "source": [
        "transform = v2.Compose([\n",
        "    v2.Resize((224, 224)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=(0.4203, 0.2800, 0.1714),\n",
        "                std=(0.2932, 0.2165, 0.1632))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h1V3QuFrmA5e"
      },
      "outputs": [],
      "source": [
        "transform_augment = v2.Compose([\n",
        "    v2.Resize((224,224)),\n",
        "    v2.RandomRotation(degrees=15, expand=False, fill=0),\n",
        "    v2.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.ToImage(),                                 # convert PIL â†’ tensor\n",
        "    v2.ToDtype(torch.float32, scale=True),        # scale to [0,1]\n",
        "    v2.Normalize(mean=(0.4203, 0.2800, 0.1714),\n",
        "                std=(0.2932, 0.2165, 0.1632)),\n",
        "    v2.GaussianNoise(mean=0.0, sigma=0.01, clip=True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwfluaBSmZAD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "YgTirjJnmDV_",
        "outputId": "0d556947-2fd6-46a5-8a8d-1d7846363f9e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/eye-diseases-classification/dataset'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1645664627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_to_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/eye-diseases-classification/dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_to_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/eye-diseases-classification/dataset'"
          ]
        }
      ],
      "source": [
        "path_to_dataset = \"/kaggle/input/eye-diseases-classification/dataset\"\n",
        "dataset= ImageFolder(root=path_to_dataset, transform=None)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b8tmuY8mpkE"
      },
      "source": [
        "### Separating train-validation-test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AykQJ-I5mrgK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV5TfNCcmtOQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Total number of data items: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WmTZ4I3m0kb"
      },
      "outputs": [],
      "source": [
        "indices = np.arange(len(dataset)) # represent each dataset item with an index for easier splitting\n",
        "labels = dataset.targets # labels of corresponding indexed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlf0nQFvm2DR"
      },
      "outputs": [],
      "source": [
        "train_idx, test_idx, y_train, y_test = train_test_split(indices, labels, train_size=0.8, stratify=labels, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SARWSaB8m3tC"
      },
      "outputs": [],
      "source": [
        "print(f\"Train size = {train_idx.shape[0]}\")\n",
        "print(f\"Test size = {test_idx.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vU4Y7DNm9Lv"
      },
      "source": [
        "- Below: Using `torch.utils.data.Subset()` for getting the final splitted dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yajYGLVBm9_n"
      },
      "outputs": [],
      "source": [
        "train_dataset_original = torch.utils.data.Subset(dataset=dataset, indices=train_idx)\n",
        "train_dataset_augment = torch.utils.data.Subset(dataset=dataset, indices=train_idx)\n",
        "# validation_dataset = torch.utils.data.Subset(dataset=dataset, indices=val_idx)\n",
        "test_dataset = torch.utils.data.Subset(dataset=dataset, indices=test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpQHEpoMnAuT"
      },
      "outputs": [],
      "source": [
        "from copy import copy\n",
        "# all subsets now point to different parent dataset\n",
        "train_dataset_original.dataset = copy(dataset)\n",
        "train_dataset_augment.dataset = copy(dataset)\n",
        "test_dataset.dataset = copy(dataset)\n",
        "# now individual transform can be applied\n",
        "train_dataset_augment.dataset.transform = transform_augment\n",
        "train_dataset_original.dataset.transform = transform\n",
        "test_dataset.dataset.transform = transform\n",
        "# validation_dataset.dataset.transform = transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy8iyjvunDxI"
      },
      "outputs": [],
      "source": [
        "train_dataset_complete = torch.utils.data.ConcatDataset([train_dataset_augment, train_dataset_original])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQfU2-LAnFUE"
      },
      "outputs": [],
      "source": [
        "len(train_dataset_complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqPUE8q7nH44"
      },
      "source": [
        "### Training the ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F36pG44LnGRO"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    '''A resnet block with skip connection'''\n",
        "    def __init__(self, in_channels:int, out_channels:int, stride:int):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features=out_channels)\n",
        "        # self.relu1 --> non-learnable torch.functional kept in forward() method\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(num_features=out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential() # in case of residual-output dimension mismatch\n",
        "        if (in_channels!=out_channels or stride!=1):\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(num_features=out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x:torch.tensor)->torch.tensor:\n",
        "        out = torch.relu(self.batchnorm1(self.conv1(x)))\n",
        "        out = self.batchnorm2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7fZa5gCnLAb"
      },
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    '''A ResNet18 model'''\n",
        "    def __init__(self, num_classes:int=4):\n",
        "        super().__init__()\n",
        "        self.in_channels=64\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
        "        # relu, maxpool, in forward() method\n",
        "\n",
        "        self.layer1 = self.make_blocks(ResBlock, 64, 2, 1)\n",
        "        self.layer2 = self.make_blocks(ResBlock, 128, 2, 2)\n",
        "        self.layer3 = self.make_blocks(ResBlock, 256, 2, 2)\n",
        "        self.layer4 = self.make_blocks(ResBlock, 512, 2, 2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_blocks(self, block:ResBlock, out_channels:int, num_blocks:int, stride:int):\n",
        "        '''make a residual block'''\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for i in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride=i))\n",
        "            self.in_channels=out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x:torch.tensor)->torch.tensor:\n",
        "        out = self.batchnorm1(self.conv1(x))\n",
        "        out = F.max_pool2d(torch.relu(out), 2)\n",
        "        out = F.dropout(out, p=0.1, training=self.training)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = F.dropout(out, p=0.1, training=self.training)\n",
        "        out = self.layer2(out)\n",
        "        out = F.dropout(out, p=0.2, training=self.training)\n",
        "        out = self.layer3(out)\n",
        "        out = F.dropout(out, p=0.3, training=self.training)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avg_pool(out) # returns tensor of shape (B,512,1,1)\n",
        "        out = out.view(out.shape[0], -1) # turn into shape (no. of images in a batch, 512)\n",
        "        out = F.dropout(out, p=0.5, training=self.training)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
